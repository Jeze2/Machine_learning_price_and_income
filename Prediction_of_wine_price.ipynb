{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Jeze2/Machine_learning_price_and_income/blob/main/project2.ipynb",
      "authorship_tag": "ABX9TyNhfwD+Dbap+PdBQpFF7WVH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeze2/Machine_learning_price_and_income/blob/main/Prediction_of_wine_price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AruzivT7xZY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "import pandas as pd\n",
        "fpath = \"/content/drive/MyDrive/StudentFolders16wk20wk/CodingDojo (1)/02-MachineLearning/Week07/Data/wines_SPA.csv\"\n",
        "df = pd.read_csv(fpath)\n",
        "\n"
      ],
      "metadata": {
        "id": "vC5QDD2o8SBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "l1I0Qf4J8WdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "89Nyd8lt_6pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Source of data\n",
        "- The was collected by me using web scraping from different sources (from wine specialized pages to supermarkets)\n",
        "2. Brief description of data\n",
        "- This dataset is related to red variants of spanish wines. The dataset describes several popularity and description metrics their effect on it's quality.\n",
        "3. What is the target?\n",
        "- The target would be the price\n",
        "4. What does one row represent? (A person? A business? An event? A product?)\n",
        "- It represents the product which is the wine\n",
        "5. Is this a classification or regression problem?\n",
        "- This appears to be a regression problem rather than a classification problem.\n",
        "The target variable we are trying to predict is 'price', which is a continuous numerical value.  because we are trying to predict the continuous target variable 'price' based on other numeric features, this is a regression problem\n",
        "6. How many features does the data have?\n",
        "- The data has 11 features\n",
        "7. How many rows are in the dataset?\n",
        "- 7499\n",
        "8. What, if any, challenges do you foresee in cleaning, exploring, or modeling this dataset?\n",
        "- Missing or incomplete data: The dataset may contain missing values or incomplete entries, which could require data cleaning techniques such as imputation or removal of incomplete records.\n",
        "- Class imbalance: The dataset mentions that the classes (quality ratings) are ordered and not balanced. This means there may be an unequal distribution of samples across different quality ratings."
      ],
      "metadata": {
        "id": "CK_W94eh-K6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Data types"
      ],
      "metadata": {
        "id": "YqG1kSKidrV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping NA values\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "rXdZQvkpdWR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing N.V. with -1\n",
        "df['year'] = df['year'].replace('N.V.', '-1')"
      ],
      "metadata": {
        "id": "n5VTHPD5dGlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing datatype\n",
        "df[\"year\"] = df[\"year\"].astype(int)"
      ],
      "metadata": {
        "id": "kbEHmsjKcbvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['year'].dtypes\n"
      ],
      "metadata": {
        "id": "9hUkL072cOhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use .select_dtypes to view only object columns\n",
        "df.select_dtypes"
      ],
      "metadata": {
        "id": "Q_3h_o1pG7J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Unwanted Columns"
      ],
      "metadata": {
        "id": "ftj1NQuYG617"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping unnecessary columns\n",
        "df.drop(df.columns [[0,1,5,6]], axis=1, inplace = True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2hKqsHc7IH6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicate Rows"
      ],
      "metadata": {
        "id": "vjWePoOxJtEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a filter called \"duplicated_rows\" that identifies duplicated rows\n",
        "\n",
        "duplicated_rows = df.duplicated()\n",
        "# Get the sum of duplicated_rows filter to see the number of duplicate rows\n",
        "\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "XPoc0KTmJovW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a new filter called \"duplicated_rows_all\" that identifies ALL\n",
        "# of the duplicated rows\n",
        "duplicated_rows_all = df.duplicated(keep=False)\n",
        "\n",
        "# Calculate the sum of the duplicated_rows_all filter\n",
        "duplicated_rows_all.sum()"
      ],
      "metadata": {
        "id": "q3bvg4EoJzOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Obtain the sum of duplicated to confirm all have been dropped\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "S6AnrVKqJ5rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Missing Values"
      ],
      "metadata": {
        "id": "LBHm29zJKNKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of null values for all columns\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "-r9RVWprJos9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save list of categorical column name.\n",
        "cat_cols = df.select_dtypes('object').columns\n",
        "cat_cols"
      ],
      "metadata": {
        "id": "8xO5GSGQJoqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the list of cat columns to fill null values with MISSING\n",
        "\n",
        "df[cat_cols] = df[cat_cols].fillna(\"MISSING\")\n",
        "df[cat_cols].isna().sum()"
      ],
      "metadata": {
        "id": "tMGeN71QJonT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save list of numeric column names\n",
        "num_cols = df.select_dtypes('number').columns\n",
        "num_cols"
      ],
      "metadata": {
        "id": "2ZBTSnXWKedo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the list of num columns to fill null values with -1\n",
        "df[num_cols] = df[num_cols].fillna(\"-1\")\n",
        "df[num_cols].isna().sum()"
      ],
      "metadata": {
        "id": "8Ab2wiggJogQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Consistent Values"
      ],
      "metadata": {
        "id": "t9lZ8eHZKo8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Save a list of object columns\n",
        "cat_cols = df.select_dtypes('object').columns"
      ],
      "metadata": {
        "id": "i9yRoir7KqKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remake the list of string columns\n",
        "cat_cols = df.select_dtypes('object').columns\n",
        "\n",
        "# loop through the list of string columns\n",
        "\n",
        "  # print the value counts for the column\n",
        "\n",
        "  # Print an empty line for readability\n",
        "for col in cat_cols:\n",
        "  print(f\"Value Counts for {col}\")\n",
        "  print(df[col].value_counts())\n",
        "\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "b36aCQlbKqOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "Rn7FEZC16288"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"UNIVARIATE PLOTTING FUNCTIONS FOR EDA\"\"\"\n",
        "# Add the print statements to the function\n",
        "def explore_categorical(df, x, fillna = True, placeholder = 'MISSING',\n",
        "                        figsize = (6,4), order = None):\n",
        "  \"\"\"Creates a seaborn countplot with the option to temporarily fill missing values\n",
        "  Prints statements about null values, cardinality, and checks for\n",
        "  constant/quasi-constant features.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "  # Make a copy of the dataframe and fillna\n",
        "  temp_df = df.copy()\n",
        "  # Before filling nulls, save null value counts and percent for printing\n",
        "  null_count = temp_df[x].isna().sum()\n",
        "  null_perc = null_count/len(temp_df)* 100\n",
        "  # fillna with placeholder\n",
        "  if fillna == True:\n",
        "    temp_df[x] = temp_df[x].fillna(placeholder)\n",
        "  # Create figure with desired figsize\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  # Plotting a count plot\n",
        "  sns.countplot(data=temp_df, x=x, ax=ax, order=order)\n",
        "  # Rotate Tick Labels for long names\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "  # Add a title with the feature name included\n",
        "  ax.set_title(f\"Column: {x}\", fontweight='bold')\n",
        "\n",
        "  # Fix layout and show plot (before print statements)\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Print null value info\n",
        "  print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "  # Print cardinality info\n",
        "  nunique = temp_df[x].nunique()\n",
        "  print(f\"- Unique Values: {nunique}\")\n",
        "  # First find value counts of feature\n",
        "  val_counts = temp_df[x].value_counts(dropna=False)\n",
        "  # Define the most common value\n",
        "  most_common_val = val_counts.index[0]\n",
        "  # Define the frequency of the most common value\n",
        "  freq = val_counts.values[0]\n",
        "  # Calculate the percentage of the most common value\n",
        "  perc_most_common = freq / len(temp_df) * 100\n",
        "  # Print the results\n",
        "  print(f\"- Most common value: '{most_common_val}' occurs {freq} times ({round(perc_most_common,2)}%)\")\n",
        "  # print message if quasi-constant or constant (most common val more than 98% of data)\n",
        "  if perc_most_common > 98:\n",
        "    print(f\"\\n- [!] Warning: '{x}' is a constant or quasi-constant feature and should be dropped.\")\n",
        "  else:\n",
        "    print(\"- Not constant or quasi-constant.\")\n",
        "  return fig, ax\n",
        "\n",
        "\n",
        "# TO DO: add the new print statements from explore_categorical\n",
        "def explore_numeric(df, x, figsize=(6,5) ):\n",
        "  \"\"\"Creates a seaborn histplot and boxplot with a share x-axis,\n",
        "  Prints statements about null values, cardinality, and checks for\n",
        "  constant/quasi-constant features.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "\n",
        "  ## Save null value counts and percent for printing\n",
        "  null_count = df[x].isna().sum()\n",
        "  null_perc = null_count/len(df)* 100\n",
        "\n",
        "\n",
        "  ## Making our figure with gridspec for subplots\n",
        "  gridspec = {'height_ratios':[0.7,0.3]}\n",
        "  fig, axes = plt.subplots(nrows=2, figsize=figsize,\n",
        "                           sharex=True, gridspec_kw=gridspec)\n",
        "  # Histogram on Top\n",
        "  sns.histplot(data=df, x=x, ax=axes[0])\n",
        "\n",
        "  # Boxplot on Bottom\n",
        "  sns.boxplot(data=df, x=x, ax=axes[1])\n",
        "\n",
        "  ## Adding a title\n",
        "  axes[0].set_title(f\"Column: {x}\", fontweight='bold')\n",
        "\n",
        "  ## Adjusting subplots to best fill Figure\n",
        "  fig.tight_layout()\n",
        "\n",
        "  # Ensure plot is shown before message\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Print null value info\n",
        "  print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "  # Print cardinality info\n",
        "  nunique = df[x].nunique()\n",
        "  print(f\"- Unique Values: {nunique}\")\n",
        "\n",
        "\n",
        "  # Get the most most common value, its count as # and as %\n",
        "  most_common_val_count = df[x].value_counts(dropna=False).head(1)\n",
        "  most_common_val = most_common_val_count.index[0]\n",
        "  freq = most_common_val_count.values[0]\n",
        "  perc_most_common = freq / len(df) * 100\n",
        "\n",
        "  print(f\"- Most common value: '{most_common_val}' occurs {freq} times ({round(perc_most_common,2)}%)\")\n",
        "\n",
        "  # print message if quasi-constant or constant (most common val more than 98% of data)\n",
        "  if perc_most_common > 98:\n",
        "    print(f\"\\n- [!] Warning: '{x}' is a constant or quasi-constant feature and should be dropped.\")\n",
        "  else:\n",
        "    print(\"- Not constant or quasi-constant.\")\n",
        "  return fig, axes\n"
      ],
      "metadata": {
        "id": "P-g0Azg564xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"MULTIVARIATE PLOTTING FUNCTIONS VS. NUMERIC TARGET\"\"\"\n",
        "\n",
        "def plot_categorical_vs_target(df, x, y='rating',figsize=(6,4),\n",
        "                            fillna = True, placeholder = 'MISSING',\n",
        "                            order = None):\n",
        "  \"\"\"Plots a combination of a seaborn barplot of means combined with\n",
        "  a seaborn stripplot to show the spread of the data.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "  # Make a copy of the dataframe and fillna\n",
        "  temp_df = df.copy()\n",
        "  # fillna with placeholder\n",
        "  if fillna == True:\n",
        "    temp_df[x] = temp_df[x].fillna(placeholder)\n",
        "\n",
        "  # or drop nulls prevent unwanted 'nan' group in stripplot\n",
        "  else:\n",
        "    temp_df = temp_df.dropna(subset=[x])\n",
        "  # Create the figure and subplots\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Barplot\n",
        "  sns.barplot(data=temp_df, x=x, y=y, ax=ax, order=order, alpha=0.6,\n",
        "              linewidth=1, edgecolor='black', errorbar=None)\n",
        "\n",
        "  # Boxplot\n",
        "  sns.stripplot(data=temp_df, x=x, y=y, hue=x, ax=ax,\n",
        "                order=order, hue_order=order, legend=False,\n",
        "                edgecolor='white', linewidth=0.5,\n",
        "                size=3,zorder=0)\n",
        "  # Rotate xlabels\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "  # Add a title\n",
        "  ax.set_title(f\"{x} vs. {y}\", fontweight='bold')\n",
        "  fig.tight_layout()\n",
        "  return fig, ax\n",
        "\n",
        "\n",
        "def plot_numeric_vs_target(df, x, y='rating',\n",
        "                           figsize=(6,4),\n",
        "                           ):\n",
        "  \"\"\"Plots a seaborn regplot with Pearson's correlation (r) added\n",
        "  to the title.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "  # Calculate the correlation\n",
        "  corr = df[[x,y]].corr().round(2)\n",
        "  r = corr.loc[x,y]\n",
        "\n",
        "  # Plot the data\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  scatter_kws={'ec':'white','lw':1,'alpha':0.8}\n",
        "  sns.regplot(data=df, x=x, y=y, ax=ax, scatter_kws=scatter_kws)\n",
        "\n",
        "  ## Add the title with the correlation\n",
        "  ax.set_title(f\"{x} vs. {y} (r = {r})\", fontweight='bold')\n",
        "\n",
        "  # Make sure the plot is shown before the print statement\n",
        "  plt.show()\n",
        "\n",
        "  return fig, ax\n"
      ],
      "metadata": {
        "id": "vfBqJ9YF67cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "e9GAQttfR5hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wine price distribution\n",
        "ax = df['price'].hist(bins=10, edgecolor='black')\n",
        "\n",
        "ax.set_ylabel('Number of Wines')\n",
        "ax.set_xlabel('Wine Price (USD)')\n",
        "ax.set_title('Distribution of Wine Prices')\n"
      ],
      "metadata": {
        "id": "cz7jmKuNR1Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation heatmap of all of the numeric columns.\n",
        "corr = df.corr(numeric_only=True)\n",
        "ax = sns.heatmap(corr,annot=True, cmap='Oranges')\n",
        "ax.set(title='Correlation Heatmap');\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9LbbKBbYb7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First visualization\n",
        "plot_categorical_vs_target(df, 'acidity','price')"
      ],
      "metadata": {
        "id": "9J4pV2bNAbZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Visualization 1 - Acidity vs price\n",
        "- This scatterplot shows the relationship between acidity levels and price for the wines in the dataset. Each data point represents an individual wine sample. Acidity is shown on the x-axis while the price of each wine is displayed on the y-axis. The plot reveals a moderate positive correlation between acidity and price levels. Wines with higher acidity tend to be priced higher than wines with lower acidity."
      ],
      "metadata": {
        "id": "btP23wDL_5M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explore_numeric(df,'rating');"
      ],
      "metadata": {
        "id": "ERMVYJY-BJZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Visualization 2 - Rating\n",
        "- The bar chart shows that the rating of wines are between 4.3 and 4.5. While there is some rating over or less than that, the overall trend is in between those two numbers The visualization provides an intuitive understanding that wines have been getting good ratings."
      ],
      "metadata": {
        "id": "TqBe1IRVACtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization from README\n",
        "plot_categorical_vs_target(df, 'type','price')"
      ],
      "metadata": {
        "id": "Duk3PRQWhvwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing"
      ],
      "metadata": {
        "id": "Og-tUnq-tYk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "6_thlzYAt6xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y).\n",
        "X = df.drop(columns='price')\n",
        "y = df['price']\n",
        "## Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "mTzLYfH3tLM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing X_train and y_train\n",
        "display(X_train.head(3), y_train.head(3))"
      ],
      "metadata": {
        "id": "9QIWo6ZEuKc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING PIPELINE FOR NUMERIC DATA\n",
        "# Save list of column names\n",
        "num_cols = X_train.select_dtypes(\"number\").columns\n",
        "print(\"Numeric Columns:\", num_cols)\n",
        "# instantiate preprocessors\n",
        "impute_median = SimpleImputer(strategy='median')\n",
        "scaler = StandardScaler()\n",
        "# Make a numeric preprocessing pipeline\n",
        "num_pipe = make_pipeline(impute_median, scaler)\n",
        "num_pipe"
      ],
      "metadata": {
        "id": "rVPobj7WuQOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a numeric tuple for ColumnTransformer\n",
        "num_tuple = ('numeric', num_pipe, num_cols)\n",
        "num_tuple"
      ],
      "metadata": {
        "id": "x73EajT8uXNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING PIPELINE FOR ONE-HOT-ENCODED DATA\n",
        "# Save list of column names\n",
        "ohe_cols = X_train.select_dtypes('object').columns\n",
        "print(\"OneHotEncoder Columns:\", ohe_cols)\n",
        "# Instantiate the individual preprocessors\n",
        "impute_na = SimpleImputer(strategy='constant', fill_value = \"NA\")\n",
        "ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "# Make pipeline with imputer and encoder\n",
        "ohe_pipe = make_pipeline(impute_na, ohe_encoder)\n",
        "ohe_pipe"
      ],
      "metadata": {
        "id": "DkRJ8vgPuaMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_tuple = ('categorical', ohe_pipe, ohe_cols)\n",
        "ohe_tuple"
      ],
      "metadata": {
        "id": "_qHGNIUhuftF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the make column transformer\n",
        "col_transformer = ColumnTransformer([num_tuple,\n",
        "\n",
        "                                       ohe_tuple],\n",
        "                                       remainder='drop', verbose_feature_names_out=False)"
      ],
      "metadata": {
        "id": "6k363rFmuhvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit\n",
        "col_transformer.fit(X_train)"
      ],
      "metadata": {
        "id": "Te6z3JTWulmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the X_train and the X_test\n",
        "X_train_proc = col_transformer.transform(X_train)\n",
        "X_test_proc = col_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "M6UHzEI2upmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW import\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n"
      ],
      "metadata": {
        "id": "1B7OKeEIurpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add custom functions (from snippets if you have saved it)\n",
        "\n",
        "def regression_metrics(y_true, y_pred, label='', verbose = True, output_dict=False):\n",
        "  # Get metrics\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  mse = mean_squared_error(y_true, y_pred)\n",
        "  rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "  r_squared = r2_score(y_true, y_pred)\n",
        "  if verbose == True:\n",
        "    # Print Result with Label and Header\n",
        "    header = \"-\"*60\n",
        "    print(header, f\"Regression Metrics: {label}\", header, sep='\\n')\n",
        "    print(f\"- MAE = {mae:,.3f}\")\n",
        "    print(f\"- MSE = {mse:,.3f}\")\n",
        "    print(f\"- RMSE = {rmse:,.3f}\")\n",
        "    print(f\"- R^2 = {r_squared:,.3f}\")\n",
        "  if output_dict == True:\n",
        "      metrics = {'Label':label, 'MAE':mae,\n",
        "                 'MSE':mse, 'RMSE':rmse, 'R^2':r_squared}\n",
        "      return metrics\n",
        "\n",
        "def evaluate_regression(reg, X_train, y_train, X_test, y_test, verbose = True,\n",
        "                        output_frame=False):\n",
        "  # Get predictions for training data\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "\n",
        "  # Call the helper function to obtain regression metrics for training data\n",
        "  results_train = regression_metrics(y_train, y_train_pred, verbose = verbose,\n",
        "                                     output_dict=output_frame,\n",
        "                                     label='Training Data')\n",
        "  print()\n",
        "  # Get predictions for test data\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "  # Call the helper function to obtain regression metrics for test data\n",
        "  results_test = regression_metrics(y_test, y_test_pred, verbose = verbose,\n",
        "                                  output_dict=output_frame,\n",
        "                                    label='Test Data' )\n",
        "\n",
        "  # Store results in a dataframe if ouput_frame is True\n",
        "  if output_frame:\n",
        "    results_df = pd.DataFrame([results_train,results_test])\n",
        "    # Set the label as the index\n",
        "    results_df = results_df.set_index('Label')\n",
        "    # Set index.name to none to get a cleaner looking result\n",
        "    results_df.index.name=None\n",
        "    # Return the dataframe\n",
        "    return results_df.round(3)"
      ],
      "metadata": {
        "id": "C-41zqF8u2Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "Wc4nl-c4yTMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate default random forest model\n",
        "rf = RandomForestRegressor(random_state = 42)\n",
        "# Model Pipeline\n",
        "rf_pipe = make_pipeline(col_transformer, rf)"
      ],
      "metadata": {
        "id": "L2VgSg7pEqz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model pipeline on the training data only\n",
        "rf_pipe.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "rJzqT9YMBQ8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use custom function to evaluate default model\n",
        "evaluate_regression(rf_pipe, X_train, y_train, X_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "nJDv-zvoFor4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define param grid with options to try\n",
        "params = {'randomforestregressor__max_depth': [None,10,15,20],\n",
        "          'randomforestregressor__n_estimators':[10,100,150,200],\n",
        "          'randomforestregressor__min_samples_leaf':[2,3,4],\n",
        "          'randomforestregressor__max_features':['sqrt','log2',None],\n",
        "          'randomforestregressor__oob_score':[True,False],\n",
        "          }"
      ],
      "metadata": {
        "id": "UwkNqe47GOlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "iwS_zEGYyud7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the gridsearch\n",
        "gridsearch = GridSearchCV(rf_pipe, params, n_jobs=-1, cv = 3, verbose=1)\n",
        "# Fit the gridsearch on training data\n",
        "gridsearch.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "rBzfrsQMGVxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and refit best model\n",
        "best_rf = gridsearch.best_estimator_\n",
        "evaluate_regression(best_rf, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "_zzgv0C4GXkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping unnecessary columns\n",
        "df.drop(df.columns [[4]], axis=1, inplace = True)\n",
        "df.head"
      ],
      "metadata": {
        "id": "t1kDtIzrlcVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "Rlni8qIqXUfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "# Fit & transform data.\n",
        "scaled_df = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "h1M9VpIrVC70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate, fit & transform data using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(scaled_df)"
      ],
      "metadata": {
        "id": "xInDw3SOVDju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "DQTRnrp-3Tsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor #NEW import\n"
      ],
      "metadata": {
        "id": "X9ATTg7O2cjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model 1 - with PCA transformed data\n",
        "knn_pca = KNeighborsRegressor()\n",
        "knn_pca.fit(pcs, y)"
      ],
      "metadata": {
        "id": "V5slcKTr2cdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2 - with original scaled data\n",
        "knn_scaled = KNeighborsRegressor()\n",
        "knn_scaled.fit(scaled_df, y)"
      ],
      "metadata": {
        "id": "co8JaVHz3O2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_pred_pca = knn_pca.predict(pcs)"
      ],
      "metadata": {
        "id": "Ns9xknB55Xm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Non-PCA model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_pca = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred_pca)\n",
        "print(\"PCA MSE:\", mse)"
      ],
      "metadata": {
        "id": "Tt3aatUS50pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_pred_scaled = knn_scaled.predict(scaled_df)"
      ],
      "metadata": {
        "id": "v-b7iJfM5XeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "model.fit(X_train_pca, y_train)\n",
        "y_pred_scaled = model.predict(X_test_pca)\n",
        "\n",
        "mse_pca = mean_squared_error(y_test, y_pred_scaled)\n",
        "print(\"NON-PCA MSE:\", mse_pca)"
      ],
      "metadata": {
        "id": "6i5A3smj6g9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Random Forest regressor achieved the lowest Mean Squared Error (MSE) of 27,104 on the training data and 50,012 on the test data compared to other models, indicating it has the best price prediction accuracy that minimizes squared error loss. Random Forest is the optimal production model for this problem."
      ],
      "metadata": {
        "id": "Ga2h-Dyv7mY3"
      }
    }
  ]
}